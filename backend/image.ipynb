{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os # para manipular archivos\n",
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'asl_data_set'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'base_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [8], line 14\u001b[0m\n\u001b[1;32m      6\u001b[0m datagen \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mpreprocessing\u001b[39m.\u001b[39mimage\u001b[39m.\u001b[39mImageDataGenerator(\n\u001b[1;32m      7\u001b[0m     \u001b[39m# rescale = 1./255, # reescalamos las imagenes\u001b[39;00m\n\u001b[1;32m      8\u001b[0m     validation_split\u001b[39m=\u001b[39m\u001b[39m0.2\u001b[39m \u001b[39m# separar los datos en 2 grupos, uno para entrenar y otro para validar\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[39m# de manera convencional, el 80% de los datos se usan para entrenar y el 20% para validar\u001b[39;00m\n\u001b[1;32m     10\u001b[0m )\n\u001b[1;32m     12\u001b[0m \u001b[39m# generador de datos de entrenamiento\u001b[39;00m\n\u001b[1;32m     13\u001b[0m train_generator \u001b[39m=\u001b[39m datagen\u001b[39m.\u001b[39mflow_from_directory( \n\u001b[0;32m---> 14\u001b[0m     base_dir, \u001b[39m# directorio base que contiene las carpetas de las imagenes\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     target_size\u001b[39m=\u001b[39m(IMAGE_SIZE, IMAGE_SIZE),  \u001b[39m# convertir las imagenes a un tamaño uniforme de 224x224\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     batch_size \u001b[39m=\u001b[39m BATCH_SIZE, \u001b[39m# las imagenes se procesan en lotes de 64 en la red neuronal\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mtraining\u001b[39m\u001b[39m'\u001b[39m \u001b[39m# el nombre que asignamos a la carpeta de entrenamiento\u001b[39;00m\n\u001b[1;32m     18\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[39m# generador de datos de validacion\u001b[39;00m\n\u001b[1;32m     21\u001b[0m val_generator \u001b[39m=\u001b[39m datagen\u001b[39m.\u001b[39mflow_from_directory(  \n\u001b[1;32m     22\u001b[0m     base_dir, \n\u001b[1;32m     23\u001b[0m     target_size\u001b[39m=\u001b[39m(IMAGE_SIZE, IMAGE_SIZE),\n\u001b[1;32m     24\u001b[0m     batch_size\u001b[39m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     25\u001b[0m     subset\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mvalidation\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m     26\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'base_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# preprocesamiento de los datos, es decir, preparar los datos en crudo para construir y entrenar modelos\n",
    "IMAGE_SIZE = 224 # tamaño de las imagenes \n",
    "BATCH_SIZE = 64  # cantidad de images que se van a procesar en cada paso\n",
    "\n",
    "# preprocsamiento de la imagen\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    # rescale = 1./255, # reescalamos las imagenes\n",
    "    validation_split=0.2 # separar los datos en 2 grupos, uno para entrenar y otro para validar\n",
    "    # de manera convencional, el 80% de los datos se usan para entrenar y el 20% para validar\n",
    ")\n",
    "\n",
    "# generador de datos de entrenamiento\n",
    "train_generator = datagen.flow_from_directory( \n",
    "    base_dir, # directorio base que contiene las carpetas de las imagenes\n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),  # convertir las imagenes a un tamaño uniforme de 224x224\n",
    "    batch_size = BATCH_SIZE, # las imagenes se procesan en lotes de 64 en la red neuronal\n",
    "    subset='training' # el nombre que asignamos a la carpeta de entrenamiento\n",
    ")\n",
    "\n",
    "# generador de datos de validacion\n",
    "val_generator = datagen.flow_from_directory(  \n",
    "    base_dir, \n",
    "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
    "    batch_size=BATCH_SIZE,\n",
    "    subset='validation'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': 0, 'B': 1, 'C': 2, 'D': 3, 'E': 4, 'F': 5, 'G': 6, 'H': 7, 'I': 8, 'J': 9, 'K': 10, 'L': 11, 'M': 12, 'N': 13, 'O': 14, 'P': 15, 'Q': 16, 'R': 17, 'S': 18, 'T': 19, 'U': 20, 'V': 21, 'W': 22, 'X': 23, 'Y': 24, 'Z': 25, 'del': 26, 'space': 27}\n"
     ]
    }
   ],
   "source": [
    "# A continuacion, tenemos que crear un archivo de etiquetas que contendra todas nuestras etiquetas (importante para Flutter)\n",
    "print(train_generator.class_indices) # imprime las etiquetas de las imagenes del dataset\n",
    "labels = '\\n'.join(sorted(train_generator.class_indices.keys())) # imprime las keys como etiquetas en un archivo de texto llamado labels.txt\n",
    "with open('labels.txt', 'w') as f: # escribe en el archivo labels.txt, y si no existe, lo crea, y si existe, lo sobreescribe. (eso es lo que 'w' es para)\n",
    "    f.write(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 16:30:30.716604: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:306] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-11-16 16:30:30.717125: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:272] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/mobilenet_v2/mobilenet_v2_weights_tf_dim_ordering_tf_kernels_1.0_224_no_top.h5\n",
      "9406464/9406464 [==============================] - 1s 0us/step\n"
     ]
    }
   ],
   "source": [
    "# construimos una red neuronal usando el metodo de transferencia de aprendizaje donde tomamos una red neuronal preentrenada llamada MobileNetV2\n",
    "#  que es una arquitectura de red neuronal convolucional que busca funcionar bien en dispositivos moviles y puede predecir hasta 80 clases diferentes\n",
    "# tendremos una red neuronal base en la parte superior de la cual agregaremos una red neuronal preentrenada para que prediga las clases que queremos\n",
    "IMG_SHAPE = (IMAGE_SIZE, IMAGE_SIZE, 3) \n",
    "base_model = tf.keras.applications.MobileNetV2( # MobileNetV2 es una red neuronal convolucional preentrenada\n",
    "    input_shape=IMG_SHAPE,\n",
    "    include_top=False, # esto parara los pesos de la red neuronal en la parte superior, porque no queremos que se entrenen, porque ya estan entrenados\n",
    "    weights='imagenet'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable=False # esto congelara todos los neuronas para nuestro modelo base\n",
    "model = tf.keras.Sequential([ # redes neuronales actuan en una secuencia de capas, asi que agregamos capas a medida que lo necesitamos\n",
    "  base_model,\n",
    "  tf.keras.layers.Conv2D(32,3, activation = 'relu'), # esta capa crea un kernel de convolucion que se convoluciona con la entrada de la capa para producir una tensor de salida\n",
    "  tf.keras.layers.Dropout(0.2), # esta capa evita el sobreajuste, es decir, que la red neuronal sea demasiado precisa hasta el punto de que solo pueda reconocer imagenes que estan presentes en el conjunto de datos\n",
    "  tf.keras.layers.GlobalAveragePooling2D(), # esta capa calcula el promedio de salida de cada mapa de caracteristicas en la capa anterior, lo que reduce los datos significativamente y prepara el modelo para la capa final\n",
    "  tf.keras.layers.Dense(28, # numero de clases que queremos predecir\n",
    "                        activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(), # Adam es un optimizador popular, diseñado específicamente para el entrenamiento de redes neuronales profundas\n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 16:33:43.692655: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-11-16 16:33:44.545767: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050/1050 [==============================] - ETA: 0s - loss: 1.8301 - accuracy: 0.4410"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-16 16:35:35.155524: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:114] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1050/1050 [==============================] - 137s 126ms/step - loss: 1.8301 - accuracy: 0.4410 - val_loss: 1.8442 - val_accuracy: 0.4216\n",
      "Epoch 2/10\n",
      "1050/1050 [==============================] - 131s 125ms/step - loss: 0.9927 - accuracy: 0.6804 - val_loss: 1.6681 - val_accuracy: 0.4569\n",
      "Epoch 3/10\n",
      "1050/1050 [==============================] - 130s 124ms/step - loss: 0.8139 - accuracy: 0.7342 - val_loss: 1.4752 - val_accuracy: 0.5232\n",
      "Epoch 4/10\n",
      "1050/1050 [==============================] - 132s 126ms/step - loss: 0.7165 - accuracy: 0.7652 - val_loss: 1.6163 - val_accuracy: 0.5033\n",
      "Epoch 5/10\n",
      "1050/1050 [==============================] - 131s 124ms/step - loss: 0.6450 - accuracy: 0.7860 - val_loss: 1.6275 - val_accuracy: 0.5104\n",
      "Epoch 6/10\n",
      "1050/1050 [==============================] - 130s 124ms/step - loss: 0.5937 - accuracy: 0.8035 - val_loss: 1.4543 - val_accuracy: 0.5464\n",
      "Epoch 7/10\n",
      "1050/1050 [==============================] - 130s 123ms/step - loss: 0.5535 - accuracy: 0.8172 - val_loss: 1.5317 - val_accuracy: 0.5402\n",
      "Epoch 8/10\n",
      "1050/1050 [==============================] - 130s 124ms/step - loss: 0.5242 - accuracy: 0.8243 - val_loss: 1.5055 - val_accuracy: 0.5386\n",
      "Epoch 9/10\n",
      "1050/1050 [==============================] - 130s 124ms/step - loss: 0.4928 - accuracy: 0.8362 - val_loss: 1.4702 - val_accuracy: 0.5584\n",
      "Epoch 10/10\n",
      "1050/1050 [==============================] - 130s 124ms/step - loss: 0.4743 - accuracy: 0.8423 - val_loss: 1.3749 - val_accuracy: 0.5657\n"
     ]
    }
   ],
   "source": [
    "epochs = 10 # a mayor numero de epochs, mas preciso sera el modelo, pero tambien puede causar sobreajuste, si es demasiado alto\n",
    "history = model.fit(\n",
    "    train_generator, \n",
    "    epochs = epochs, \n",
    "    validation_data=val_generator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Function `_wrapped_model` contains input name(s) mobilenetv2_1.00_224_input with unsupported characters which will be renamed to mobilenetv2_1_00_224_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: assets\n",
      "WARNING:absl:Function `signature_wrapper` contains input name(s) mobilenetv2_1.00_224_input with unsupported characters which will be renamed to mobilenetv2_1_00_224_input in the SavedModel.\n",
      "WARNING:absl:Found untraced functions such as restored_function_body, restored_function_body, restored_function_body, restored_function_body, restored_function_body while saving (showing 5 of 53). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/w9/47hfrj5s20n22dxytnnn2mjh0000gn/T/tmpd0f_nov3/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /var/folders/w9/47hfrj5s20n22dxytnnn2mjh0000gn/T/tmpd0f_nov3/assets\n",
      "2022-11-16 16:59:42.026766: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:362] Ignored output_format.\n",
      "2022-11-16 16:59:42.026921: W tensorflow/compiler/mlir/lite/python/tf_tfl_flatbuffer_helpers.cc:365] Ignored drop_control_dependency.\n",
      "2022-11-16 16:59:42.029963: I tensorflow/cc/saved_model/reader.cc:45] Reading SavedModel from: /var/folders/w9/47hfrj5s20n22dxytnnn2mjh0000gn/T/tmpd0f_nov3\n",
      "2022-11-16 16:59:42.049095: I tensorflow/cc/saved_model/reader.cc:89] Reading meta graph with tags { serve }\n",
      "2022-11-16 16:59:42.049111: I tensorflow/cc/saved_model/reader.cc:130] Reading SavedModel debug info (if present) from: /var/folders/w9/47hfrj5s20n22dxytnnn2mjh0000gn/T/tmpd0f_nov3\n",
      "2022-11-16 16:59:42.091948: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:354] MLIR V1 optimization pass is not enabled\n",
      "2022-11-16 16:59:42.106683: I tensorflow/cc/saved_model/loader.cc:229] Restoring SavedModel bundle.\n",
      "2022-11-16 16:59:42.402289: I tensorflow/cc/saved_model/loader.cc:213] Running initialization op on SavedModel bundle at path: /var/folders/w9/47hfrj5s20n22dxytnnn2mjh0000gn/T/tmpd0f_nov3\n",
      "2022-11-16 16:59:42.486882: I tensorflow/cc/saved_model/loader.cc:305] SavedModel load for tags { serve }; Status: success: OK. Took 457291 microseconds.\n",
      "2022-11-16 16:59:42.707968: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n"
     ]
    }
   ],
   "source": [
    "# con nuestra red neuronal entrenada con tensorflow y keras, podemos exportarla\n",
    "saved_model_dir = '' # directorio donde se guardara el modelo\n",
    "tf.saved_model.save(model, saved_model_dir) # guarda el modelo en el directorio\n",
    "\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir) \n",
    "tflite_model = converter.convert() # convierte el modelo en un modelo de tensorflow lite, que pueda usar flutter \n",
    "with open('model.tflite', 'wb') as f: # guardamos el modelo en un archivo llamado model.tflite, y si no existe, lo crea, y si existe, lo sobreescribe. (eso es lo que 'wb' es para)\n",
    "  f.write(tflite_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargamos el modelo en memoria \n",
    "path = 'model.tflite' # ruta del modelo\n",
    "interpreter = tf.lite.Interpreter(model_path=path) \n",
    "interpreter.allocate_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M\n"
     ]
    }
   ],
   "source": [
    "# cargamos la imagen\n",
    "path = 'asl_test/M_test.jpg'\n",
    "img = tf.keras.preprocessing.image.load_img(path , target_size=(IMAGE_SIZE, IMAGE_SIZE)) # cargamos la imagen de prueba\n",
    "img_array = tf.keras.preprocessing.image.img_to_array(img) # convertimos la imagen a un array\n",
    "img_array = tf.expand_dims(img_array, 0) # expandimos las dimensiones del array\n",
    "\n",
    "\n",
    "# predecimos la imagen \n",
    "input_details = interpreter.get_input_details() # obtenemos los detalles de la entrada\n",
    "output_details = interpreter.get_output_details() # obtenemos los detalles de la salida\n",
    "\n",
    "# convertimos a float32\n",
    "input_shape = input_details[0]['shape']\n",
    "input_data = np.array(img_array, dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke() # invocamos el modelo\n",
    "output_data = interpreter.get_tensor(output_details[0]['index']) # obtenemos la salida\n",
    "# print(output_data) # imprimimos la salida\n",
    "\n",
    "# obtenemos la etiqueta de la prediccion\n",
    "with open('labels.txt', 'r') as f:\n",
    "    labels = [line.strip() for line in f.readlines()]\n",
    "prediction = np.argmax(output_data) # obtenemos el indice de la prediccion\n",
    "print(labels[prediction]) # imprimimos la etiqueta de la prediccion\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "504e7e5b2741c53adad9e7ce964919d2b35311b8c691a892782b3bc51b096c66"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
